---
title: "Practical Machine Learning Project - Predicting Performance of Exercise Movements"
author: "Jesse Beaumont - January 8, 2017"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(data.table)
library(dplyr)
library(tidyr)
library(caret)
require(randomForest)

set.seed(57987432)
```


##Executive Summary - Predicting Exercise Performance

The purpose of this study predict the manner in which an individual exercise was performed.

A considerable amount of data is collected from wearable devices which capture how much specific activities are performed. 
For example, an individual may be interested in recording how many weight lifting repetitions they accomplished.
However, it should be possible to attest to whether or not they did each repetition properly and record  the number of mistakes.

We shall model movemements collected from accelerometers during these exercises located at 4 locations on the each participant:

   1. The belt
   2. The upper arm
   3. The forearm
   4. The weight itself.

Six participants performed 10 repetitions of unilateral dumbbell biceps curls in 
5 different manners: 

   1. Exactly according to the specification (Class A),
   2. Throwing the elbows to the front (Class B),
   3. Lifting the dumbbell only halfway (Class C),
   4. Lowering the dumbbell only halfway (Class D)
   5. Throwing the hips to the front (Class E).

Class 'A' corresponds to the exercise being performed correctly. The remaining activity classes correpond to common mistakes.
Each method is specified in the source data by the variable "classe".


_The data used was kindly made available from http://groupware.les.inf.puc-rio.br/har .  Thank you to the collaborators involved during the study._


###Building Our Model

We began our analysis by exploring the source data. The data contains a mixture of both granular
time series of movements as well as summary statistics pertaining to the entire range of motion per 
repetition.  We begin my eliminating the sparse values from our data.

```{r, echo=TRUE, cache = FALSE}
# Load the source data.
#options(stringsAsFactors = TRUE)
pml_training <- read.csv("./pml-training.csv", header = TRUE)
pml_testing  <- read.csv("./pml-testing.csv",  header = TRUE)
```

Since our test data only contains specific points and not the full range of motion in a repetition, 
we shall exclude these rows from our training data denoted by the "new window" value.

```{r, echo=TRUE, cache = FALSE}
pml_training <- subset(pml_training, new_window == "no")
```

Also, we shall exclude the sparsity of the data caused by empty columns.

```{r, echo=TRUE, cache = FALSE}
temp <- as.data.table(pml_training)
temp <- temp[,which(unlist(lapply(temp, function(x)!all( is.na(x)) ))), with=F]

# clean residual levels that are of 1 distinct value
temp <- droplevels(temp)
temp <- temp[, (names(temp)[which(sapply(temp, uniqueN) == 1)]) := NULL]
```

The test cases we will be classifying do not account for the full range of motion and are granular
points in time recorded within a repetition. Our model will be compiled respective of the same level
so we remove the time related metrics. Also, since our model should be capable of predicting what action
was performed, "num_window", should absolutely be removed as it was left in the test data and has a direct
correlation to the classe in the data available for training.

```{r, echo=TRUE, cache = FALSE}
# remove the row ordinal value
# remove the timeseries details
temp <- temp[,-c('X', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'num_window')]
sourcedata <- as.data.frame(temp)
```



###Training Data

We shall train our model using 75% of the observations in our source data.
Later, we will use the remaining observations to cross-validate
our predictive model.

```{r, echo=TRUE}
train_part <- createDataPartition(y=sourcedata$classe, p=3/4)[[1]]
training <- sourcedata[train_part,]
validation <- sourcedata[-train_part,]
```



###Model using Random Forest

The model is compiled using random forest.
Random forest estimates a test set error internally. Each decision tree 
is constructed using a different bootstrap sample from the training data.
Each tree leaves uses approx 2/3 of the data in the bootstrap sample. The remaining 1/3
are left out and may be used to calculate the expected out-of-sample error.  This is 
referred to as the "out-of-bag" (OOB) data. 

```{r, echo=TRUE}
rf.model <- randomForest(classe ~ ., data=training, mtry=2, ntree=500)
print(rf.model)
```



###Cross Validation

Using the validation data prepared earlier, we will cross-validate our model.

```{r, echo=TRUE}
predictions <- predict(rf.model, validation)
confusionMatrix(predictions, validation[,c("classe")])
```



###Expected Out-of-Sample Error

We shall calculate our out-of-sample error by calculating the percentage 
of misclassified results.  This is equivalent to 1 - accuracy of the validation
predictions.

```{r, echo=TRUE}
count_of_incorrect_predictions <- sum(predictions != validation$classe)
count_of_predictions <- length(predictions)
OOSE <- count_of_incorrect_predictions/count_of_predictions
paste("Expected Out-of-Sample Error: ", round(100 * OOSE, 2), "%", sep="")
```



###Variable Importance

Observing variable importance, we can see that "roll belt"" contributes to the most variance in the
outcome.

```{r, echo=TRUE, fig.width=7, fig.height=6}
varImpPlot(rf.model)
```



###Prediction Results

The test data, containing 20 test cases, provided is applied to the predictive model.

```{r, echo=TRUE}
prediction_results <- predict(rf.model, pml_testing)
prediction_results
```

